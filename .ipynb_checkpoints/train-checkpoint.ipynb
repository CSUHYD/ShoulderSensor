{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor2Angle\n",
    "\n",
    "### Step\n",
    "1. Data Pre-processing\n",
    "    - Minmax 归一化*\n",
    "    - 滤波*\n",
    "    - 加入DataLoader\n",
    "2. Model Defination\n",
    "    - LSTM\n",
    "    - 滑动窗口\n",
    "3. Training\n",
    "\n",
    "4. Eval\n",
    "\n",
    "## 1. Data Pre-processing\n",
    "https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler, SubsetRandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision import transforms, utils\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(Sampler):\n",
    "    r\"\"\"Wraps another sampler to yield a mini-batch of indices.\n",
    "    Args:\n",
    "        sampler (Sampler): Base sampler.\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "    \"\"\"\n",
    "# 批次采样\n",
    "    def __init__(self, sampler, batch_size, drop_last):\n",
    "        if not isinstance(sampler, Sampler):\n",
    "            raise ValueError(\"sampler should be an instance of \"\n",
    "                             \"torch.utils.data.Sampler, but got sampler={}\"\n",
    "                             .format(sampler))\n",
    "        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n",
    "                batch_size <= 0:\n",
    "            raise ValueError(\"batch_size should be a positive integeral value, \"\n",
    "                             \"but got batch_size={}\".format(batch_size))\n",
    "        if not isinstance(drop_last, bool):\n",
    "            raise ValueError(\"drop_last should be a boolean value, but got \"\n",
    "                             \"drop_last={}\".format(drop_last))\n",
    "        self.sampler = sampler\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch = []\n",
    "        for idx in self.sampler:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.drop_last:\n",
    "            return len(self.sampler) // self.batch_size\n",
    "        else:\n",
    "            return (len(self.sampler) + self.batch_size - 1) // self.batch_size    \n",
    "\n",
    "        \n",
    "class Sensor2AngleDataset(Dataset):\n",
    "    \"\"\"Sensor to Angle Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path, seq_len, cut, do_standardize=True, do_filter=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "        \"\"\"\n",
    "        self.x, self.y = sliding_windows(dataset_path, seq_len, cut, do_standardize, do_filter)\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert(self.x.shape[0] == self.y.shape[0])\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        # sensor data\n",
    "        sensor = self.x[idx]\n",
    "        # angle data\n",
    "        angle = self.y[idx]\n",
    "        sample = {'sensor': sensor, 'angle': angle}\n",
    "            \n",
    "        return sample\n",
    "\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sensor, angle = sample['sensor'], sample['angle']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        return {'sensor': torch.from_numpy(sensor),\n",
    "                'angle': torch.from_numpy(angle)}\n",
    "\n",
    "def read_csv(dataset_path):\n",
    "    data_frame = pd.read_csv(dataset_path)\n",
    "    num_sample = data_frame.shape[0]\n",
    "    data_frame.dropna(inplace=True)\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "    data_frame = data_frame.iloc[:,-14:]\n",
    "    print(f'清洗数据：({data_frame.shape[0]}/{num_sample})')\n",
    "    return data_frame\n",
    "\n",
    "def sliding_windows(dataset_path, seq_len, cut=0, do_standardize=True, do_filter=True):\n",
    "    data_frame = read_csv(dataset_path)\n",
    "    sensor = np.array([data_frame.iloc[:, -5:]]).astype('float').squeeze()[cut:]\n",
    "    angle = np.array([data_frame.iloc[:, -14:-8]]).astype('float').squeeze()[cut:]\n",
    "    print(angle.shape)\n",
    "    if do_standardize:\n",
    "        scaler = StandardScaler()                          #实例化\n",
    "#         scaler = MinMaxScaler()\n",
    "        sensor = scaler.fit_transform(sensor)              #使用fit_transform(data)一步达成结果\n",
    "#         angle = scaler.fit_transform(angle)\n",
    "#         scaler.fit(sensor)                      #fit，本质是生成均值和方差\n",
    "#         print(scaler.mean_)                     #查看均值的属性mean_\n",
    "#         print(scaler.var_)                      #查看方差的属性var_\n",
    "#         x_std = scaler.transform(sensor)        #通过接口导出结果\n",
    "#         print(x_std.mean())                     #导出的结果是一个数组，用mean()查看均值\n",
    "#         print(x_std.std())                      #用std()查看方差\n",
    "#         scaler.inverse_transform(x_std)         #使用inverse_transform逆转标准化\n",
    "    if do_filter:\n",
    "        for i in range(sensor.shape[1]):\n",
    "            sensor[:,i] = filter(sensor[:,i], 51, 2, do_plot=True)\n",
    "        for i in range(angle.shape[1]):\n",
    "            angle[:,i] = filter(angle[:,i], 51, 2, do_plot=True)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    assert(sensor.shape[0] == angle.shape[0])\n",
    "    num_sample = sensor.shape[0]\n",
    "    for i in range(num_sample-seq_len+1):\n",
    "        _x = sensor[i:(i+seq_len)]\n",
    "        _y = angle[i+seq_len-1]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "        \n",
    "    return [np.array(x), np.array(y)]\n",
    "\n",
    "\n",
    "def filter(data, window, k, do_plot=True):\n",
    "    result = savgol_filter(data, window, k, mode= 'nearest')\n",
    "    if do_plot:\n",
    "        plt.plot(data, color='r')\n",
    "        plt.plot(result, 'b', label = 'savgol')\n",
    "        plt.show()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = 'data/processing/QianquForward.csv'\n",
    "# data_frame = pd.read_csv(dataset_path)\n",
    "# num_sample = data_frame.shape[0]\n",
    "# data_frame.dropna(inplace=True)\n",
    "# data_frame.reset_index(drop=True, inplace=True)\n",
    "# data_frame = data_frame.iloc[:,-14:]\n",
    "# sensor = np.array([data_frame.iloc[:, -5:]]).astype('float').squeeze()\n",
    "# angle = np.array([data_frame.iloc[:, -14:-8]]).astype('float').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'data/processing/Backward.csv'\n",
    "batch_size = 16\n",
    "validation_split = .2\n",
    "random_seed= 43\n",
    "seq_len = 5\n",
    "\n",
    "# torch.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset = Sensor2AngleDataset(dataset_path, seq_len, cut=200, do_standardize=True, do_filter=True)\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    " \n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SequentialSampler(train_indices)\n",
    "test_sampler = SequentialSampler(test_indices)\n",
    "\n",
    "# sc = MinMaxScaler()\n",
    "# dataset = sc.fit_transform(training_set)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           sampler=train_sampler, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=0, \n",
    "                                           drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader( dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           sampler=test_sampler, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=0, \n",
    "                                           drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate(train_loader):\n",
    "    print(data['sensor'].size())\n",
    "    print(data['angle'].size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Defination\n",
    "#### Time Series Regression Model:\n",
    "- LSTM\n",
    "- Transformer\n",
    "- WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, inp_dim, mid_dim, num_layers, out_dim, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(inp_dim, mid_dim, num_layers, batch_first=True).to(device)\n",
    "        # inp_dim 是LSTM输入张量的维度，我们已经根据我们的数据确定了这个值是3\n",
    "        # mid_dim 是LSTM三个门 (gate) 的网络宽度，也是LSTM输出张量的维度\n",
    "        # num_layers 是使用两个LSTM对数据进行预测，然后将他们的输出堆叠起来。\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(mid_dim * seq_len, out_dim)\n",
    "#             nn.Linear(30, 50), nn.ReLU(),\n",
    "#             nn.Linear(50, 100), nn.ReLU(),\n",
    "#             nn.Linear(100, 50), nn.ReLU(),\n",
    "#             nn.Linear(50, out_dim)\n",
    "        )  # regression\n",
    "        self.fc = nn.Linear(mid_dim * seq_len, out_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers, batch_size, out_dim).to(device) #(num_layers,batch,output_size)\n",
    "        c0 = torch.zeros(num_layers, batch_size, out_dim).to(device) #(num_layers,batch,output_size)\n",
    "        output, (hn, cn) = self.rnn(x, (h0, c0))\n",
    "        output = output.contiguous().view(batch_size, -1)\n",
    "        output = self.reg(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def get_parameter_number(net):\n",
    "    total_num = sum(p.numel() for p in net.parameters())\n",
    "    trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    return {'Total': total_num, 'Trainable': trainable_num}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_dim = 5\n",
    "mid_dim = 6\n",
    "num_layers = 2\n",
    "out_dim = 6\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "lstm = LSTM(batch_size, inp_dim, mid_dim, num_layers, out_dim, seq_len).to(device)\n",
    "print(get_parameter_number(lstm))\n",
    "# criterion = torch.nn.MSELoss(reduction='sum')    # mean-squared error for regression\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 100, gamma = 0.1, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Train the model\n",
    "min_loss = 100000#随便设置一个比较大的数\n",
    "min_save_epoch = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for index, data in enumerate(train_loader):\n",
    "        trainX = data['sensor'].float().to(device)\n",
    "        trainY = data['angle'].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm(trainX)\n",
    "        loss = criterion(outputs, trainY)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#     scheduler.step()\n",
    "    if loss < min_loss and epoch > min_save_epoch:\n",
    "        min_loss = loss\n",
    "        print(\"save model\")\n",
    "        torch.save(lstm.state_dict(), 'model.pth')\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label = list()\n",
    "predict = list()\n",
    "loss_list = list()\n",
    "\n",
    "lstm.load_state_dict(torch.load('model.pth'), strict=False)\n",
    "lstm.eval()\n",
    "for index, data in enumerate(train_loader):\n",
    "    x_ = data['sensor'].float().to(device)\n",
    "    y_ = data['angle'].float().data.numpy()\n",
    "    p_ = lstm(x_).cpu().data.numpy()\n",
    "    label.append(y_)\n",
    "    predict.append(p_)\n",
    "    \n",
    "    trainX = data['sensor'].float().to(device)\n",
    "    trainY = data['angle'].float().to(device)\n",
    "    outputs = lstm(trainX)\n",
    "    loss = criterion(outputs, trainY)\n",
    "    loss_list.append(loss.item())\n",
    "    \n",
    "print('mean loss:', np.array(loss_list).mean())\n",
    "label = np.array(label).reshape(-1,6)\n",
    "predict = np.array(predict).reshape(-1,6)\n",
    "for i in range(6):\n",
    "    figure(figsize=(16, 9), dpi=100)\n",
    "    plt.plot(label[:,i], color='r')\n",
    "    plt.plot(predict[:,i], color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
